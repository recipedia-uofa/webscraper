# Webscraper

## Installing Python Dependencies

First, create a virtual environment in the project root directory

`$ python3 -m venv .env`

Then activate the virtual environment with

`$ source .env/bin/activate`

Finally, install all required dependencies with

`$ pip3 install -r requirements.txt`

## Installing Node dependencies

First, ensure [node](https://nodejs.org/en/) and [yarn](https://yarnpkg.com/) are installed.

To install all dependencies, run

`$ yarn install`

## Testing
To run tests, run the following command in the project root after all dependencies for python are installed:

`$ python3 -m coverage run --source=. -m pytest`

To generate a coverage report, run

`$ python3 -m coverage report`

## Scraper

The scraper is responsible for downloading raw html files for recipes from allrecipes.com. The html files will be stored at a timestamped folder in the download folder: `./download/<timestamp>`

Following is the synopsis of command line arguments that are accepted by the script. Note that both `startId` and `endId` are inclusive.

`$ node scraper.js -h`

```
usage: scraper.js [-h] [-s startId] [-e endId]

Script to download raw recipe htmls from allrecipes.com

Optional arguments:
  -h, --help            Show this help message and exit.
  -s startId, --startId startId
                        The ID of the first recipe to download
  -e endId, --endId endId
                        The ID of the last recipe to download
```

For example, to download recipes 20000 to 70000, run

`$ node scraper.js -s 20000 -e 70000`

##### Caveats
- The default values for startId is 6663 and the default value for endId is 269344 now.
- The script prints `skip: recipe <id> don't have a nutrition button` when there is no nutrition button in the html page.
- The script retries each recipe at most 3 (maxRetry) times.
- After failing to connect for 3 (maxConsecutiveFailures) times, the script will terminate.

## HtmlParser

The HtmlParser parses the raw recipe htmls downloaded by the Scraper. Following is the synopsis of the script

```
usage: parser.py [-h] [-o OUTPUT] path

Script to parse raw recipe htmls

positional arguments:
  path                  path to the folder of the htmls

optional arguments:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        path to the output file
```

To parse the htmls generated by the parser, run

`$ python3 parser.py path/to/the/folder/that/contains/downloaded/recipe/htmls`

The script by default outputs a csv file called `htmls.csv`, which will be used in the later to build the database.

## DatabaseBuilder

The DatabaseBuilder builds the database from the csv file generated in the HtmlParser. Following is the synopsis of the script:

```
usage: build_database.py [-h] [-i INPUT] [-o OUTPUT] [-m MODEL]

Script that builds the .rdf file from a csv file of recipes

optional arguments:
  -h, --help            show this help message and exit
  -i INPUT, --input INPUT
                        path to the input csv file
  -o OUTPUT, --output OUTPUT
                        path to the output file
  -m MODEL, --model MODEL
                        path to a serialized nutriscore model
```
For example, run the following command to build `recipedia.rdf` which is used by the recipedia repository.

`$ python3 -i htmls.csv -o recipedia.rdf`
